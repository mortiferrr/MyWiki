**Линейная регрессия**, как и следует из названия, решает [[Основные задачи машинного обучения|задачу регрессии]]. Иными словами, наша цель — построить систему, которая принимает на входе вектор $x∈R^{n}$, а на выходе порождает скалярное значение $y∈R$. Результатом линейной регрессии является **линейная функция входных данных**. Обозначим $\hat{y}$ — значение $y$, предсказанное моделью. Определим результат модели в виде
$$\hat{y}=w^{T}x$$
где $w∈R^{n}$ — **вектор параметров**. 
**Параметры** — это величины, управляющие **поведением системы**. В данном случае коэффициент $w_{i}$ — коэффициент, на который нужно умножить признак $x_{i}$ перед включением в сумму вкладов всех признаков. Иными словами, $w$ — **набор весов**, описывающих **влияние отдельных признаков** на результат предсказания.
Пусть имеется матрица плана с $m$ примерами, которые мы будем использовать не для обучения, а только для оценки качества модели. Имеется также вектор меток, содержащий правильные значения $y$ для каждого из этих примеров. Поскольку этот набор будет использоваться только для контроля качества, назовем его **тестовым набором**. Обозначим **матрицу плана** $X^{(test)}$, а вектор меток регрессии — $y^{test}$.
Один из способов измерения качества модели — вычислить **среднеквадратичную ошибку** модели на тестовом наборе. Если вектор $\hat{y}^{(test)}$ содержит предсказания модели на тестовом наборе, то среднеквадратичная ошибка определяется по формуле:
$$MSE_{test} = \frac{1}{m}\sum_{i}({\hat{y}^{(test)}-y^{(test)}})^{2}_{i}$$
Эта мера ошибки обращается в ноль тогда, когда $\hat{y}^{(test)} = y^{(test)}$. Кроме того эту формулу можно перезаписать в следующем виде:
$$MSE_{test} = \frac{1}{m}|| \hat{y}^{(test)} - y^{(test)} ||^2_{2},$$
поэтому ошибка тем больше, чем больше евклидово расстояние между предсказаниями и метками.
Мы должны спроектировать алгоритм [[Определение машинного обучения|машинного обучения]], который улучшает веса $w$ таким образом, что $MSE_{test}$ уменьшается по мере того, как алгоритм получает новый [[Опыт|опыт]], наблюдая обучающий (тестовый) набор. Интуитивно понятный способ добиться этой цели — минимизировать среднеквадратическую ошибку на обучающем наборе, $MSE_{train}$.
Для этого, для начала, приравняем градиент функции потерь к нулю:
$$\nabla_{w}MSE_{train} = 0$$
Решая получившееся уравнение получаем:
$$w=(X^{(train)T}X^{(train)})^{-1}X^{(train)T}y^{(train)}$$
Система уравнений, решение которой дает выше выведенная формула, называется **нормальными уравнениями**. Вычисление выражения этой формулы и есть простой алгоритм обучения линейное регрессии в действии.
Отметим, что часто термином **линейная регрессия** обозначают несколько более сложную модель с одним дополнительным параметром — свободным членом $b$. В этой модели $\hat{y}=w^{T}x + b$, поэтому отображение параметров на предсказания по-прежнему описывается линейной функцией, но отображение признаков на предсказания теперь является **аффинной функцией**. Это обобщение означает, что график предсказанной модели — прямая, которая не обязана проходить через начало координат. Вместо добавления параметра $b$ можно продолжать пользоваться моделью одних лишь весов, но дополнить вектор $x$ элементом, который всегда равен единице. Вес, соответствующий этому элементу, играет роль свободного члена.
Свободный член $b$ часто называют параметром **смещения (bias)** аффинного преобразования. Это связано с тем, что результат преобразования смещается в сторону b, если входных данных нет вообще.


